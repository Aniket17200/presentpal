{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Q1M9tVmC8LCk","outputId":"1496273c-4d40-4425-8bc3-e490a4d3807b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: python-pptx==0.6.23 in /usr/local/lib/python3.11/dist-packages (0.6.23)\n","Requirement already satisfied: edge-tts==6.1.12 in /usr/local/lib/python3.11/dist-packages (6.1.12)\n","Requirement already satisfied: pydub==0.25.1 in /usr/local/lib/python3.11/dist-packages (0.25.1)\n","Collecting fastapi==0.109.2\n","  Using cached fastapi-0.109.2-py3-none-any.whl.metadata (25 kB)\n","Collecting pyngrok==7.0.0\n","  Using cached pyngrok-7.0.0-py3-none-any.whl\n","Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx==0.6.23) (5.3.1)\n","Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx==0.6.23) (11.1.0)\n","Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from python-pptx==0.6.23) (3.2.2)\n","Requirement already satisfied: aiohttp>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from edge-tts==6.1.12) (3.11.13)\n","Requirement already satisfied: certifi>=2023.11.17 in /usr/local/lib/python3.11/dist-packages (from edge-tts==6.1.12) (2025.1.31)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.109.2) (2.10.6)\n","Collecting starlette<0.37.0,>=0.36.3 (from fastapi==0.109.2)\n","  Using cached starlette-0.36.3-py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.109.2) (4.12.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyngrok==7.0.0) (6.0.2)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts==6.1.12) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts==6.1.12) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts==6.1.12) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts==6.1.12) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts==6.1.12) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts==6.1.12) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts==6.1.12) (1.18.3)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.109.2) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.109.2) (2.27.2)\n","Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from starlette<0.37.0,>=0.36.3->fastapi==0.109.2) (3.7.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi==0.109.2) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi==0.109.2) (1.3.1)\n","Using cached fastapi-0.109.2-py3-none-any.whl (92 kB)\n","Using cached starlette-0.36.3-py3-none-any.whl (71 kB)\n","Installing collected packages: pyngrok, starlette, fastapi\n","  Attempting uninstall: pyngrok\n","    Found existing installation: pyngrok 7.2.3\n","    Uninstalling pyngrok-7.2.3:\n","      Successfully uninstalled pyngrok-7.2.3\n","  Attempting uninstall: starlette\n","    Found existing installation: starlette 0.46.0\n","    Uninstalling starlette-0.46.0:\n","      Successfully uninstalled starlette-0.46.0\n","  Attempting uninstall: fastapi\n","    Found existing installation: fastapi 0.115.11\n","    Uninstalling fastapi-0.115.11:\n","      Successfully uninstalled fastapi-0.115.11\n","Successfully installed fastapi-0.109.2 pyngrok-7.0.0 starlette-0.36.3\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:pyngrok.process.ngrok:t=2025-03-03T13:24:23+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"]},{"name":"stdout","output_type":"stream","text":["🔊 Fast Audio Service Using Edge TTS (PrabhatNeural)\n"]},{"name":"stderr","output_type":"stream","text":["INFO:     Started server process [672]\n","INFO:     Waiting for application startup.\n","INFO:     Application startup complete.\n","INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"]},{"name":"stdout","output_type":"stream","text":["🚀 Fast Audio Service Ready at: https://6643-34-106-25-186.ngrok-free.app/generate-audio/\n","INFO:     203.192.225.140:0 - \"POST /generate-audio/ HTTP/1.1\" 200 OK\n","INFO:     203.192.225.140:0 - \"POST /generate-audio/ HTTP/1.1\" 200 OK\n","INFO:     203.192.225.140:0 - \"POST /generate-audio/ HTTP/1.1\" 200 OK\n","INFO:     203.192.225.140:0 - \"POST /generate-audio/ HTTP/1.1\" 200 OK\n","INFO:     203.192.225.140:0 - \"POST /generate-audio/ HTTP/1.1\" 200 OK\n"]}],"source":["# Install Required Libraries\n","!pip install python-pptx==0.6.23 edge-tts==6.1.12 pydub==0.25.1 fastapi==0.109.2 pyngrok==7.0.0 uvicorn nest-asyncio\n","\n","# System Dependencies for Audio Processing\n","!apt-get install -y ffmpeg -qqq\n","\n","import os\n","import uvicorn\n","import nest_asyncio\n","import re\n","import zipfile\n","import asyncio\n","import shutil\n","from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks\n","from fastapi.responses import FileResponse\n","from pptx import Presentation\n","from edge_tts import Communicate\n","from pydub import AudioSegment\n","from pyngrok import ngrok\n","\n","# Apply nest_asyncio to allow running in environments with existing event loops\n","nest_asyncio.apply()\n","\n","# Ngrok Authentication Token (Replace with yours)\n","NGROK_AUTH_TOKEN = \"2tLdFeGUdnzFr4sqdtNUhgni7of_3pPNuvJ5y368fT8HnwRPE\"\n","ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n","\n","# FastAPI App\n","app = FastAPI()\n","\n","print(\"🔊 Fast Audio Service Using Edge TTS (PrabhatNeural)\")\n","\n","def clean_text(text):\n","    \"\"\"Removes bullets and extra spaces for faster processing\"\"\"\n","    return re.sub(r'[•\\u2022\\u25E6]', '', text).strip()\n","\n","async def process_slide(slide, index, output_folder):\n","    \"\"\"Processes each slide in parallel for faster audio generation\"\"\"\n","    try:\n","        slide_text = [clean_text(shape.text) for shape in slide.shapes if hasattr(shape, \"text\") and shape.text.strip()]\n","        if not slide_text:\n","            return None  # Skip empty slides\n","\n","        full_text = \" \".join(slide_text)\n","        temp_mp3 = f\"{output_folder}/temp_{index}.mp3\"\n","        output_wav = f\"{output_folder}/slide_{index}.wav\"\n","\n","        # Generate TTS audio with optimized settings\n","        communicate = Communicate(full_text, \"en-IN-PrabhatNeural\", rate=\"+20%\")  # Increase speed by 20%\n","        await communicate.save(temp_mp3)\n","\n","        # Convert and optimize audio (reduced bitrate for faster processing)\n","        AudioSegment.from_mp3(temp_mp3).set_frame_rate(44100).set_channels(1).export(output_wav, format=\"wav\", bitrate=\"128k\")\n","        os.remove(temp_mp3)\n","        return output_wav\n","    except Exception as e:\n","        print(f\"Error processing slide {index}: {str(e)}\")\n","        return None\n","\n","async def process_pptx(file_path):\n","    \"\"\"Processes PPTX slides in parallel\"\"\"\n","    try:\n","        prs = Presentation(file_path)\n","        output_folder = \"fast_audio_output\"\n","        os.makedirs(output_folder, exist_ok=True)\n","\n","        tasks = [process_slide(slide, i, output_folder) for i, slide in enumerate(prs.slides, 1)]\n","        audio_files = await asyncio.gather(*tasks)\n","\n","        return [file for file in audio_files if file]  # Remove None values\n","    except Exception as e:\n","        print(f\"Error processing PPTX: {str(e)}\")\n","        return []\n","\n","def create_zip(audio_files):\n","    \"\"\"Creates ZIP archive for audio files\"\"\"\n","    try:\n","        zip_path = \"fast_audio_output.zip\"\n","        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","            for file in audio_files:\n","                zipf.write(file, os.path.basename(file))\n","        return zip_path\n","    except Exception as e:\n","        print(f\"Error creating ZIP: {str(e)}\")\n","        raise HTTPException(status_code=500, detail=\"Failed to create ZIP file\")\n","\n","@app.post(\"/generate-audio/\")\n","async def create_upload_file(background_tasks: BackgroundTasks, file: UploadFile = File(...)):\n","    \"\"\"API to upload PPTX, generate audio, and return ZIP\"\"\"\n","    try:\n","        if not file.filename.endswith('.pptx'):\n","            raise HTTPException(status_code=400, detail=\"Invalid file type. Only .pptx accepted\")\n","\n","        file_path = f\"uploaded_{file.filename}\"\n","        with open(file_path, \"wb\") as buffer:\n","            buffer.write(await file.read())\n","\n","        audio_files = await process_pptx(file_path)\n","        if not audio_files:\n","            raise HTTPException(status_code=500, detail=\"No audio generated from the presentation\")\n","\n","        zip_path = create_zip(audio_files)\n","\n","        # Schedule cleanup\n","        background_tasks.add_task(shutil.rmtree, \"fast_audio_output\", ignore_errors=True)\n","        background_tasks.add_task(os.remove, file_path)\n","        background_tasks.add_task(os.remove, zip_path)\n","\n","        return FileResponse(zip_path, media_type='application/zip', filename=\"fast_audio_output.zip\")\n","    except Exception as e:\n","        print(f\"API Error: {str(e)}\")\n","        raise HTTPException(status_code=500, detail=f\"Server error: {str(e)}\")\n","\n","# Start Ngrok\n","def start_ngrok():\n","    \"\"\"Starts Ngrok Tunnel\"\"\"\n","    try:\n","        tunnel = ngrok.connect(8000)\n","        public_url = tunnel.public_url\n","        print(f\"🚀 Fast Audio Service Ready at: {public_url}/generate-audio/\")\n","    except Exception as e:\n","        print(\"❌ Error starting Ngrok:\", str(e))\n","\n","# Run the FastAPI Server with Ngrok\n","def run_server():\n","    \"\"\"Run the server\"\"\"\n","    start_ngrok()\n","    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n","\n","if __name__ == \"__main__\":\n","    run_server()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":772},"executionInfo":{"elapsed":4538,"status":"ok","timestamp":1741008228998,"user":{"displayName":"Aniket Gaikwad","userId":"05006860790385848977"},"user_tz":-330},"id":"nQMgJHhBENks","outputId":"eaba1624-8c8a-413e-c562-535dca6b4aac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.109.2)\n","Collecting fastapi\n","  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n","Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n","Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.0.0)\n","Collecting pyngrok\n","  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n","Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n","  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n","Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (3.7.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n","Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n","Downloading starlette-0.46.0-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyngrok, starlette, fastapi\n","  Attempting uninstall: pyngrok\n","    Found existing installation: pyngrok 7.0.0\n","    Uninstalling pyngrok-7.0.0:\n","      Successfully uninstalled pyngrok-7.0.0\n","  Attempting uninstall: starlette\n","    Found existing installation: starlette 0.36.3\n","    Uninstalling starlette-0.36.3:\n","      Successfully uninstalled starlette-0.36.3\n","  Attempting uninstall: fastapi\n","    Found existing installation: fastapi 0.109.2\n","    Uninstalling fastapi-0.109.2:\n","      Successfully uninstalled fastapi-0.109.2\n","Successfully installed fastapi-0.115.11 pyngrok-7.2.3 starlette-0.46.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"f2569903a6b6423eb0cacb740cc81d33","pip_warning":{"packages":["pyngrok"]}}},"metadata":{},"output_type":"display_data"}],"source":["pip install --upgrade fastapi uvicorn pyngrok\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8677,"status":"ok","timestamp":1740941303914,"user":{"displayName":"Aniket Gaikwad","userId":"05006860790385848977"},"user_tz":-330},"id":"qlHEb8Bm8aoY","outputId":"dabdb420-f839-418d-c529-703e32c8d5c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n"]}],"source":["!pip install uvicorn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2624,"status":"ok","timestamp":1741024150579,"user":{"displayName":"Aniket Gaikwad","userId":"05006860790385848977"},"user_tz":-330},"id":"o1bqnq7t8Rsm","outputId":"b47cb26e-bc89-4a83-fb13-0f96ccb1164e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-multipart\n","  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n","Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n","Installing collected packages: python-multipart\n","Successfully installed python-multipart-0.0.20\n"]}],"source":["!pip install python-multipart"]},{"cell_type":"code","source":["from google import genai\n","\n","client = genai.Client(api_key=\"AIzaSyCvrzvvFUq0IkT7vMBjlmtkwUNCbdWQ7Y0\")\n","\n","response = client.models.generate_content(\n","    model=\"gemini-2.0-flash\",\n","    contents=\"Explain how AI works\",\n",")\n","\n","print(response.text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jgWP3xAjRkIp","executionInfo":{"status":"ok","timestamp":1741026678489,"user_tz":-330,"elapsed":15269,"user":{"displayName":"Aniket Gaikwad","userId":"05006860790385848977"}},"outputId":"fffb4c0c-4b39-4a34-deed-8a9510674503"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Okay, let's break down how AI works, aiming for clarity and avoiding overly technical jargon.  We'll cover the core concepts and some common techniques.\n","\n","**The Core Idea:  Making Machines \"Smart\"**\n","\n","At its heart, Artificial Intelligence (AI) is about creating machines that can perform tasks that typically require human intelligence.  This includes things like:\n","\n","*   **Learning:**  Improving performance based on data.\n","*   **Problem-solving:**  Finding solutions to complex issues.\n","*   **Decision-making:**  Choosing the best course of action.\n","*   **Understanding Language:** Processing and interpreting human language.\n","*   **Recognizing Patterns:** Identifying trends and relationships in data.\n","*   **Perception:**  Interpreting sensory input (like images or sound).\n","\n","**Two Main Approaches to AI:**\n","\n","While the lines are blurring, we can broadly categorize AI approaches into two main camps:\n","\n","1.  **Rule-Based AI (Symbolic AI or \"Good Old-Fashioned AI\" - GOFAI):**\n","\n","    *   **How it Works:**  This approach relies on explicitly programming the machine with a set of rules and knowledge. Think of it like a complex \"if-then-else\" statement system.\n","    *   **Example:**  A chess-playing program where experts have defined rules about how to evaluate positions, which moves are legal, and strategies to follow.\n","    *   **Strengths:**  Works well in well-defined domains with clear rules.  Explainable (you can see why the AI made a certain decision).\n","    *   **Weaknesses:**  Brittle (struggles with situations not explicitly covered by the rules).  Difficult to scale to complex, real-world problems.  Requires a lot of manual effort to create and maintain the rules.\n","\n","2.  **Machine Learning (ML):**\n","\n","    *   **How it Works:** Instead of explicitly programming rules, you *train* the machine by feeding it a large amount of data.  The machine learns patterns and relationships from the data and uses those patterns to make predictions or decisions on new, unseen data.\n","    *   **Analogy:**  Think of teaching a child to recognize cats. You don't give them a list of rules (\"cats have whiskers, four legs, and pointy ears\"). You show them many pictures of cats, and they gradually learn to identify the features that are characteristic of cats.\n","    *   **Strengths:**  Can handle complex, real-world problems.  Adapts to new data and improves over time.  Requires less manual effort than rule-based systems *after* the initial setup and training.\n","    *   **Weaknesses:**  Requires large amounts of high-quality data.  Can be a \"black box\" (difficult to understand why the AI made a specific decision).  Prone to biases in the training data.\n","    *   **Subfields of Machine Learning:**  ML is a broad field with many sub-disciplines. The most prominent are:\n","\n","        *   **Supervised Learning:** The machine learns from labeled data (data where the correct answer is known).  For example, training a spam filter with emails labeled as \"spam\" or \"not spam.\"  Common tasks include classification (categorizing data) and regression (predicting a continuous value).\n","        *   **Unsupervised Learning:** The machine learns from unlabeled data (data where the correct answer is not known).  The goal is to find patterns and structures in the data.  For example, clustering customers into different segments based on their purchasing behavior.\n","        *   **Reinforcement Learning:** The machine learns by trial and error, receiving rewards or penalties for its actions.  This is often used in robotics and game playing.  Think of teaching a robot to walk by rewarding it for moving forward and penalizing it for falling.\n","        *   **Deep Learning:** A type of machine learning that uses artificial neural networks with many layers (hence \"deep\"). These networks are inspired by the structure of the human brain and are particularly good at learning complex patterns from large datasets. Deep learning powers many of the AI applications we see today, such as image recognition, natural language processing, and speech recognition.\n","\n","**Key Components & Concepts in Machine Learning (Since it's the dominant approach):**\n","\n","1.  **Data:**  The fuel that drives machine learning.  The more relevant and high-quality the data, the better the AI will perform.\n","\n","2.  **Features:**  The specific attributes or characteristics of the data that the AI uses to learn. For example, in image recognition, features might include edges, textures, and colors.  Feature engineering (selecting and transforming features) is a crucial part of the ML process.\n","\n","3.  **Algorithms:**  The mathematical formulas and procedures that the AI uses to learn from the data.  There are many different algorithms, each with its strengths and weaknesses.  Examples include:\n","    *   Linear Regression\n","    *   Logistic Regression\n","    *   Decision Trees\n","    *   Support Vector Machines (SVMs)\n","    *   Neural Networks (including Convolutional Neural Networks (CNNs) for images and Recurrent Neural Networks (RNNs) for sequences like text)\n","    *   Random Forests\n","    *   Gradient Boosting Machines (e.g., XGBoost, LightGBM)\n","\n","4.  **Model:**  The result of the training process.  It's a representation of the patterns and relationships that the AI has learned from the data.  The model can then be used to make predictions on new data.\n","\n","5.  **Training:**  The process of feeding the data to the algorithm to create the model.  The algorithm adjusts its internal parameters to minimize errors and improve its ability to make accurate predictions.\n","\n","6.  **Evaluation:**  Assessing the performance of the model on a separate dataset (the \"test set\") to see how well it generalizes to new data.  Metrics like accuracy, precision, recall, and F1-score are used to evaluate the model.\n","\n","7.  **Optimization:**  Fine-tuning the model's parameters and hyperparameters to improve its performance.  This often involves techniques like cross-validation and hyperparameter tuning.\n","\n","**The AI Development Process (Simplified):**\n","\n","1.  **Define the Problem:** What are you trying to solve with AI? Be specific.\n","2.  **Gather Data:** Collect relevant data that can be used to train the AI.\n","3.  **Prepare the Data:** Clean, preprocess, and transform the data into a format that the AI can use. This often involves handling missing values, removing outliers, and scaling the data.\n","4.  **Choose an Algorithm:** Select an appropriate algorithm based on the type of problem and the characteristics of the data.\n","5.  **Train the Model:** Feed the data to the algorithm and train the model.\n","6.  **Evaluate the Model:** Assess the performance of the model on a separate dataset.\n","7.  **Tune the Model:** Fine-tune the model's parameters and hyperparameters to improve its performance.\n","8.  **Deploy the Model:** Integrate the model into a system or application where it can be used to make predictions or decisions.\n","9.  **Monitor and Maintain:** Continuously monitor the model's performance and retrain it as needed to ensure it remains accurate and effective.\n","\n","**Important Considerations:**\n","\n","*   **Bias:**  AI models can inherit biases from the data they are trained on.  It's crucial to be aware of potential biases and take steps to mitigate them.\n","*   **Explainability:**  Understanding why an AI made a certain decision is important for trust and accountability.  \"Explainable AI\" (XAI) is a growing field focused on making AI models more transparent and understandable.\n","*   **Ethics:**  AI raises ethical concerns about privacy, fairness, and job displacement.  It's important to develop and use AI responsibly.\n","\n","**In summary:**  AI is a broad field that aims to create machines that can perform tasks that typically require human intelligence. Machine learning, particularly deep learning, is the dominant approach today, and it involves training machines on large amounts of data to learn patterns and make predictions.  The process involves data gathering, data preparation, algorithm selection, model training, evaluation, and deployment.  Ethical considerations and bias mitigation are crucial aspects of responsible AI development.\n","\n","I hope this explanation is helpful! Let me know if you have any other questions.\n","\n"]}]},{"cell_type":"code","source":["# Install Required Libraries in Colab\n","!pip install python-pptx edge-tts pydub google-generativeai\n","!apt-get install -y ffmpeg -qqq  # For pydub audio processing\n","\n","import os\n","import re\n","import zipfile\n","import asyncio\n","from google.colab import files\n","from pptx import Presentation\n","from pptx.enum.shapes import PP_PLACEHOLDER\n","from edge_tts import Communicate\n","from pydub import AudioSegment\n","import google.generativeai as genai\n","\n","# User Configuration\n","USER_FIELD_OF_STUDY = \"Computer Science\"  # Change this to your field of study\n","\n","# Google Gemini API Configuration\n","GEMINI_API_KEY = \"AIzaSyCvrzvvFUq0IkT7vMBjlmtkwUNCbdWQ7Y0\"  # Your provided key\n","genai.configure(api_key=GEMINI_API_KEY)\n","\n","def clean_text(text):\n","    \"\"\"Removes bullets and extra spaces for cleaner narration.\"\"\"\n","    return re.sub(r'[•\\u2022\\u25E6]', '', text).strip()\n","\n","def create_embedding(text):\n","    \"\"\"Simulates creating an embedding for the text (mocked).\"\"\"\n","    print(f\"Creating embedding for: {text[:50]}...\")  # Truncate for brevity\n","    return text  # Mock embedding (replace with actual embedding logic if needed)\n","\n","def generate_narration(slide_text, overall_topic, slide_title=None):\n","    \"\"\"Generates a personalized narration using Gemini API.\"\"\"\n","    if slide_title:\n","        prompt = (\n","            f\"As a student studying {USER_FIELD_OF_STUDY}, for my presentation on '{overall_topic}', \"\n","            f\"specifically on the slide titled '{slide_title}', create an engaging narration in plain text \"\n","            f\"to explain the following content to my audience: {slide_text}\"\n","        )\n","    else:\n","        prompt = (\n","            f\"As a student studying {USER_FIELD_OF_STUDY}, for my presentation on '{overall_topic}', \"\n","            f\"create an engaging narration in plain text to explain the following slide content to my audience: {slide_text}\"\n","        )\n","    try:\n","        model = genai.GenerativeModel('gemini-pro')  # Use a valid model; gemini-2.0-flash may not exist\n","        response = model.generate_content(prompt)\n","        narration_text = response.text.strip()\n","        # Additional cleaning to ensure no markdown remains\n","        narration_text = re.sub(r'[\\*\\*_]', '', narration_text)  # Remove **, *, _\n","        return narration_text\n","    except Exception as e:\n","        print(f\"Error calling Gemini API: {str(e)}\")\n","        return f\"Fallback narration: {slide_text}\"  # Fallback if API fails\n","\n","async def process_slide(slide, index, output_folder, overall_topic):\n","    \"\"\"Processes a slide: extracts text, generates narration, converts to audio.\"\"\"\n","    try:\n","        # Extract slide title\n","        slide_title = None\n","        for shape in slide.shapes:\n","            if shape.is_placeholder and shape.placeholder_format.type == PP_PLACEHOLDER.TITLE:\n","                slide_title = clean_text(shape.text)\n","                break\n","\n","        # Extract and clean all text from slide\n","        slide_text = [clean_text(shape.text) for shape in slide.shapes if hasattr(shape, \"text\") and shape.text.strip()]\n","        if not slide_text:\n","            print(f\"Slide {index}: No text found, skipping.\")\n","            return None\n","\n","        full_text = \" \".join(slide_text)\n","\n","        # Create embedding (mocked)\n","        embedded_text = create_embedding(full_text)\n","\n","        # Generate narration\n","        narration_text = generate_narration(embedded_text, overall_topic, slide_title)\n","        print(f\"Slide {index} narration: {narration_text[:50]}...\")  # Preview narration\n","\n","        # Define audio file paths\n","        temp_mp3 = f\"{output_folder}/temp_{index}.mp3\"\n","        output_wav = f\"{output_folder}/slide_{index}.wav\"\n","\n","        # Generate audio with Edge TTS\n","        communicate = Communicate(narration_text, \"en-IN-PrabhatNeural\", rate=\"+20%\")\n","        await communicate.save(temp_mp3)\n","\n","        # Convert MP3 to WAV\n","        AudioSegment.from_mp3(temp_mp3).set_frame_rate(44100).set_channels(1).export(output_wav, format=\"wav\", bitrate=\"128k\")\n","        os.remove(temp_mp3)\n","        print(f\"Slide {index}: Audio generated at {output_wav}\")\n","        return output_wav\n","    except Exception as e:\n","        print(f\"Error processing slide {index}: {str(e)}\")\n","        return None\n","\n","async def process_pptx(file_path):\n","    \"\"\"Processes the PPTX file and generates audio for all slides.\"\"\"\n","    try:\n","        prs = Presentation(file_path)\n","        output_folder = \"audio_output\"\n","        os.makedirs(output_folder, exist_ok=True)\n","\n","        # Extract overall topic from filename\n","        overall_topic = os.path.splitext(os.path.basename(file_path))[0].replace('_', ' ')\n","        print(f\"Overall topic: {overall_topic}\")\n","\n","        # Process slides\n","        tasks = [process_slide(slide, i, output_folder, overall_topic) for i, slide in enumerate(prs.slides, 1)]\n","        audio_files = await asyncio.gather(*tasks)\n","\n","        return [file for file in audio_files if file]  # Filter out None values\n","    except Exception as e:\n","        print(f\"Error processing PPTX: {str(e)}\")\n","        return []\n","\n","def create_zip(audio_files):\n","    \"\"\"Creates a ZIP file from the generated audio files.\"\"\"\n","    zip_path = \"audio_output.zip\"\n","    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","        for file in audio_files:\n","            zipf.write(file, os.path.basename(file))\n","    return zip_path\n","\n","# Upload PPTX file in Colab\n","print(\"Please upload your PPTX file (e.g., 'ChatGPT3.pptx'):\")\n","uploaded = files.upload()\n","\n","# Process the uploaded file\n","for filename in uploaded.keys():\n","    if not filename.lower().endswith('.pptx'):\n","        print(\"Error: Only .pptx files are supported!\")\n","        continue\n","\n","    pptx_file_path = filename\n","    print(f\"Processing {pptx_file_path}...\")\n","\n","    # Run async processing\n","    audio_files = asyncio.run(process_pptx(pptx_file_path))\n","\n","    if not audio_files:\n","        print(\"No audio files generated!\")\n","    else:\n","        # Create ZIP file\n","        zip_path = create_zip(audio_files)\n","        print(f\"ZIP file created at: {zip_path}\")\n","\n","        # Download the ZIP file in Colab with explicit confirmation\n","        try:\n","            files.download(zip_path)\n","            print(f\"Downloaded: {zip_path}\")\n","        except Exception as e:\n","            print(f\"Error downloading ZIP file: {str(e)}\")\n","\n","        # Clean up\n","        for file in audio_files:\n","            os.remove(file)\n","        os.rmdir(\"audio_output\")\n","        os.remove(pptx_file_path)\n","        os.remove(zip_path)\n","        print(\"Cleanup completed.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"cEAEmbExWUga","executionInfo":{"status":"ok","timestamp":1741027914394,"user_tz":-330,"elapsed":24648,"user":{"displayName":"Aniket Gaikwad","userId":"05006860790385848977"}},"outputId":"627a6104-a30a-4e6b-8976-364561b9cbef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: python-pptx in /usr/local/lib/python3.11/dist-packages (0.6.23)\n","Requirement already satisfied: edge-tts in /usr/local/lib/python3.11/dist-packages (6.1.12)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n","Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (5.3.1)\n","Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (11.1.0)\n","Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (3.2.2)\n","Requirement already satisfied: aiohttp>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from edge-tts) (3.11.13)\n","Requirement already satisfied: certifi>=2023.11.17 in /usr/local/lib/python3.11/dist-packages (from edge-tts) (2025.1.31)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.1)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.160.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.25.6)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.10.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.12.2)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts) (1.18.3)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.68.0)\n","Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n","Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.27.2)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.70.0)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.62.3)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n","Please upload your PPTX file (e.g., 'ChatGPT3.pptx'):\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-888bab0e-09d7-4142-b40c-1fb13765d74c\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-888bab0e-09d7-4142-b40c-1fb13765d74c\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving chatgpt3.pptx to chatgpt3 (1).pptx\n","Processing chatgpt3 (1).pptx...\n","Overall topic: chatgpt3 (1)\n","Creating embedding for: ChatGPT: Unraveling user Challenges & Proposing Ta...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1086.08ms\n"]},{"output_type":"stream","name":"stdout","text":["Error calling Gemini API: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n","Slide 1 narration: Fallback narration: ChatGPT: Unraveling user Chall...\n","Creating embedding for: Introduction ChatGPT: An advanced AI language mode...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 606.35ms\n"]},{"output_type":"stream","name":"stdout","text":["Error calling Gemini API: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n","Slide 2 narration: Fallback narration: Introduction ChatGPT: An advan...\n","Creating embedding for: NLP  APPLICATION IN CHATGPT Understanding Meaning:...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 606.98ms\n"]},{"output_type":"stream","name":"stdout","text":["Error calling Gemini API: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n","Slide 3 narration: Fallback narration: NLP  APPLICATION IN CHATGPT Un...\n","Creating embedding for: EVOLUTION OF  CHATGPT GPT-1 (2018):\n","First iteratio...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 582.29ms\n"]},{"output_type":"stream","name":"stdout","text":["Error calling Gemini API: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n","Slide 4 narration: Fallback narration: EVOLUTION OF  CHATGPT GPT-1 (2...\n","Creating embedding for: CONCLUSION Enhanced Decision-Making:\n","Integrating a...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 582.22ms\n"]},{"output_type":"stream","name":"stdout","text":["Error calling Gemini API: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n","Slide 5 narration: Fallback narration: CONCLUSION Enhanced Decision-M...\n","Creating embedding for: THANK YOU...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 581.75ms\n"]},{"output_type":"stream","name":"stdout","text":["Error calling Gemini API: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n","Slide 6 narration: Fallback narration: THANK YOU...\n","Slide 1: Audio generated at audio_output/slide_1.wav\n","Slide 6: Audio generated at audio_output/slide_6.wav\n","Slide 2: Audio generated at audio_output/slide_2.wav\n","Slide 4: Audio generated at audio_output/slide_4.wav\n","Slide 5: Audio generated at audio_output/slide_5.wav\n","Slide 3: Audio generated at audio_output/slide_3.wav\n","ZIP file created at: audio_output.zip\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_6f0f24cf-31c1-4edc-b8ec-07d5f01a3d1d\", \"audio_output.zip\", 9249617)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloaded: audio_output.zip\n","Cleanup completed.\n"]}]},{"cell_type":"code","source":["models = genai.list_models()\n","for model in models:\n","    print(model.name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":659},"id":"WAfeoUMpWo61","executionInfo":{"status":"ok","timestamp":1741027975659,"user_tz":-330,"elapsed":995,"user":{"displayName":"Aniket Gaikwad","userId":"05006860790385848977"}},"outputId":"ff84ed19-290b-4e25-bae1-1c48bed90f82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["models/chat-bison-001\n","models/text-bison-001\n","models/embedding-gecko-001\n","models/gemini-1.0-pro-vision-latest\n","models/gemini-pro-vision\n","models/gemini-1.5-pro-latest\n","models/gemini-1.5-pro-001\n","models/gemini-1.5-pro-002\n","models/gemini-1.5-pro\n","models/gemini-1.5-flash-latest\n","models/gemini-1.5-flash-001\n","models/gemini-1.5-flash-001-tuning\n","models/gemini-1.5-flash\n","models/gemini-1.5-flash-002\n","models/gemini-1.5-flash-8b\n","models/gemini-1.5-flash-8b-001\n","models/gemini-1.5-flash-8b-latest\n","models/gemini-1.5-flash-8b-exp-0827\n","models/gemini-1.5-flash-8b-exp-0924\n","models/gemini-2.0-flash-exp\n","models/gemini-2.0-flash\n","models/gemini-2.0-flash-001\n","models/gemini-2.0-flash-lite-001\n","models/gemini-2.0-flash-lite\n","models/gemini-2.0-flash-lite-preview-02-05\n","models/gemini-2.0-flash-lite-preview\n","models/gemini-2.0-pro-exp\n","models/gemini-2.0-pro-exp-02-05\n","models/gemini-exp-1206\n","models/gemini-2.0-flash-thinking-exp-01-21\n","models/gemini-2.0-flash-thinking-exp\n","models/gemini-2.0-flash-thinking-exp-1219\n","models/learnlm-1.5-pro-experimental\n","models/embedding-001\n","models/text-embedding-004\n","models/aqa\n","models/imagen-3.0-generate-002\n"]}]},{"cell_type":"code","source":["\n","\n","        # Clean up after download\n","        for file in audio_files:\n","            os.remove(file)\n","        os.rmdir(\"audio_output\")\n","        os.remove(pptx_file_path)\n","        os.remove(zip_path)\n","        print(\"Cleanup completed.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ME6XUkQSX7kS","executionInfo":{"status":"ok","timestamp":1741029317862,"user_tz":-330,"elapsed":13,"user":{"displayName":"Aniket Gaikwad","userId":"05006860790385848977"}},"outputId":"90f250fe-a430-4568-dcb4-be47241859a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cleanup completed.\n"]}]},{"cell_type":"code","source":["# Install Required Libraries in Colab\n","!pip install python-pptx edge-tts pydub google-generativeai\n","!apt-get install -y ffmpeg -qqq  # For pydub audio processing\n","\n","import os\n","import re\n","import zipfile\n","import asyncio\n","from google.colab import files\n","from pptx import Presentation\n","from pptx.enum.shapes import PP_PLACEHOLDER\n","from edge_tts import Communicate\n","from pydub import AudioSegment\n","import google.generativeai as genai\n","\n","# User Configuration\n","USER_FIELD_OF_STUDY = \"Computer Science\"  # Change this to your field of study\n","\n","# Google Gemini API Configuration\n","GEMINI_API_KEY = \"AIzaSyCvrzvvFUq0IkT7vMBjlmtkwUNCbdWQ7Y0\"  # Your provided key\n","genai.configure(api_key=GEMINI_API_KEY)\n","\n","def clean_text(text):\n","    \"\"\"Removes bullets and extra spaces for cleaner narration.\"\"\"\n","    return re.sub(r'[•\\u2022\\u25E6]', '', text).strip()\n","\n","def create_embedding(text):\n","    \"\"\"Simulates creating an embedding for the text (mocked).\"\"\"\n","    print(f\"Creating embedding for: {text[:50]}...\")  # Truncate for brevity\n","    return text  # Mock embedding (replace with actual embedding logic if needed)\n","\n","def generate_narration(slide_text, overall_topic, slide_title=None):\n","    \"\"\"Generates a short, impressive, and attractive narration using Gemini API.\"\"\"\n","    if slide_title:\n","        prompt = (\n","            f\"Generate a concise (40-50 words), engaging, and attractive narration that impressively explains \"\n","            f\"the slide titled '{slide_title}' in a presentation on '{overall_topic}'. \"\n","            f\"The slide content is: {slide_text}. Speak as a student in {USER_FIELD_OF_STUDY}.\"\n","        )\n","    else:\n","        prompt = (\n","            f\"Generate a concise (40-50 words), engaging, and attractive narration that impressively explains \"\n","            f\"the slide content in a presentation on '{overall_topic}'. \"\n","            f\"The slide content is: {slide_text}. Speak as a student in {USER_FIELD_OF_STUDY}.\"\n","        )\n","    try:\n","        model = genai.GenerativeModel('gemini-1.5-flash')  # Valid model\n","        response = model.generate_content(prompt)\n","        narration_text = response.text.strip()\n","        # Remove markdown for clean text\n","        narration_text = re.sub(r'[\\*\\*_]', '', narration_text)  # Remove **, *, _\n","        return narration_text\n","    except Exception as e:\n","        print(f\"Error calling Gemini API: {str(e)}\")\n","        return f\"Fallback narration: {slide_text}\"  # Fallback if API fails\n","\n","async def process_slide(slide, index, output_folder, overall_topic):\n","    \"\"\"Processes a slide: extracts text, generates narration, converts to audio.\"\"\"\n","    try:\n","        # Extract slide title\n","        slide_title = None\n","        for shape in slide.shapes:\n","            if shape.is_placeholder and shape.placeholder_format.type == PP_PLACEHOLDER.TITLE:\n","                slide_title = clean_text(shape.text)\n","                break\n","\n","        # Extract and clean all text from slide\n","        slide_text = [clean_text(shape.text) for shape in slide.shapes if hasattr(shape, \"text\") and shape.text.strip()]\n","        if not slide_text:\n","            print(f\"Slide {index}: No text found, skipping.\")\n","            return None\n","\n","        full_text = \" \".join(slide_text)\n","\n","        # Create embedding (mocked)\n","        embedded_text = create_embedding(full_text)\n","\n","        # Generate narration\n","        narration_text = generate_narration(embedded_text, overall_topic, slide_title)\n","        print(f\"Slide {index} narration: {narration_text[:50]}...\")  # Preview narration\n","\n","        # Define audio file paths\n","        temp_mp3 = f\"{output_folder}/temp_{index}.mp3\"\n","        output_wav = f\"{output_folder}/slide_{index}.wav\"\n","\n","        # Generate audio with Edge TTS using Indian male voice\n","        communicate = Communicate(narration_text, \"en-IN-PrabhatNeural\", rate=\"+10%\")\n","        await communicate.save(temp_mp3)\n","\n","        # Convert MP3 to WAV with high-quality settings\n","        AudioSegment.from_mp3(temp_mp3).set_frame_rate(44100).set_channels(1).export(output_wav, format=\"wav\")\n","        os.remove(temp_mp3)\n","        print(f\"Slide {index}: Audio generated at {output_wav}\")\n","        return output_wav\n","    except Exception as e:\n","        print(f\"Error processing slide {index}: {str(e)}\")\n","        return None\n","\n","async def process_pptx(file_path):\n","    \"\"\"Processes the PPTX file and generates audio for all slides.\"\"\"\n","    try:\n","        prs = Presentation(file_path)\n","        output_folder = \"audio_output\"\n","        os.makedirs(output_folder, exist_ok=True)\n","\n","        # Extract overall topic from filename\n","        overall_topic = os.path.splitext(os.path.basename(file_path))[0].replace('_', ' ')\n","        print(f\"Overall topic: {overall_topic}\")\n","\n","        # Process slides\n","        tasks = [process_slide(slide, i, output_folder, overall_topic) for i, slide in enumerate(prs.slides, 1)]\n","        audio_files = await asyncio.gather(*tasks)\n","\n","        return [file for file in audio_files if file]  # Filter out None values\n","    except Exception as e:\n","        print(f\"Error processing PPTX: {str(e)}\")\n","        return []\n","\n","def create_zip(audio_files):\n","    \"\"\"Creates a ZIP file from the generated audio files.\"\"\"\n","    zip_path = \"audio_output.zip\"\n","    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","        for file in audio_files:\n","            zipf.write(file, os.path.basename(file))\n","    return zip_path\n","\n","# Upload PPTX file in Colab\n","print(\"Please upload your PPTX file (e.g., 'ChatGPT3.pptx'):\")\n","uploaded = files.upload()\n","\n","# Process the uploaded file\n","for filename in uploaded.keys():\n","    if not filename.lower().endswith('.pptx'):\n","        print(\"Error: Only .pptx files are supported!\")\n","        continue\n","\n","    pptx_file_path = filename\n","    print(f\"Processing {pptx_file_path}...\")\n","\n","    # Run async processing\n","    audio_files = asyncio.run(process_pptx(pptx_file_path))\n","\n","    if not audio_files:\n","        print(\"No audio files generated!\")\n","    else:\n","        # Create ZIP file\n","        zip_path = create_zip(audio_files)\n","        print(f\"ZIP file created at: {zip_path}\")\n","\n","        # Download the ZIP file in Colab before cleanup\n","        try:\n","            files.download(zip_path)\n","            print(f\"Successfully downloaded: {zip_path}\")\n","        except Exception as e:\n","            print(f\"Error downloading ZIP file: {str(e)}\")\n","            print(\"Please manually download 'audio_output.zip' from the Colab file explorer (/content/).\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"r3U_Kpq7a_Nx","executionInfo":{"status":"ok","timestamp":1741029361578,"user_tz":-330,"elapsed":33956,"user":{"displayName":"Aniket Gaikwad","userId":"05006860790385848977"}},"outputId":"99ce763f-c48b-401f-864c-cb218fef1123"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: python-pptx in /usr/local/lib/python3.11/dist-packages (0.6.23)\n","Requirement already satisfied: edge-tts in /usr/local/lib/python3.11/dist-packages (6.1.12)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n","Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (5.3.1)\n","Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (11.1.0)\n","Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (3.2.2)\n","Requirement already satisfied: aiohttp>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from edge-tts) (3.11.13)\n","Requirement already satisfied: certifi>=2023.11.17 in /usr/local/lib/python3.11/dist-packages (from edge-tts) (2025.1.31)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.1)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.160.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.25.6)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.10.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.12.2)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.0->edge-tts) (1.18.3)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.68.0)\n","Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n","Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.27.2)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.70.0)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.62.3)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n","Please upload your PPTX file (e.g., 'ChatGPT3.pptx'):\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-6dfe5782-b66e-4fb6-aa14-4117c4e6c036\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-6dfe5782-b66e-4fb6-aa14-4117c4e6c036\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving chatgpt3.pptx to chatgpt3 (1).pptx\n","Processing chatgpt3 (1).pptx...\n","Overall topic: chatgpt3 (1)\n","Creating embedding for: ChatGPT: Unraveling user Challenges & Proposing Ta...\n","Slide 1 narration: Hey everyone!  Harsh Bande and I, Aniket Gaikwad, ...\n","Creating embedding for: Introduction ChatGPT: An advanced AI language mode...\n","Slide 2 narration: Hey everyone,  meet ChatGPT-3!  This amazing OpenA...\n","Creating embedding for: NLP  APPLICATION IN CHATGPT Understanding Meaning:...\n","Slide 3 narration: Hey everyone, so ChatGPT3's magic lies in its NLP ...\n","Creating embedding for: EVOLUTION OF  CHATGPT GPT-1 (2018):\n","First iteratio...\n","Slide 4 narration: Hey everyone!  So, ChatGPT's journey's been amazin...\n","Creating embedding for: CONCLUSION Enhanced Decision-Making:\n","Integrating a...\n","Slide 5 narration: ChatGPT-3's future is bright!  We're not just gett...\n","Creating embedding for: THANK YOU...\n","Slide 6 narration: That's all for my ChatGPT-3 overview!  We've just ...\n","Slide 6: Audio generated at audio_output/slide_6.wav\n","Slide 3: Audio generated at audio_output/slide_3.wav\n","Slide 5: Audio generated at audio_output/slide_5.wav\n","Slide 1: Audio generated at audio_output/slide_1.wav\n","Slide 2: Audio generated at audio_output/slide_2.wav\n","Slide 4: Audio generated at audio_output/slide_4.wav\n","ZIP file created at: audio_output.zip\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_bcfb64ad-7e08-4100-9e5e-6cad50def451\", \"audio_output.zip\", 9426314)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Successfully downloaded: audio_output.zip\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPTkT/N9hPPYmcis+5+cMdt"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}